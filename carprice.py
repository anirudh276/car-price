# -*- coding: utf-8 -*-
"""carprice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Ilpl9RztGbPXmPGDFis72S8BRAXt02g
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import linear_model
from sklearn.linear_model import LinearRegression

data = pd.read_csv('cars_price.csv')

column=['symboling','normalized-losses','make','fuel-type','aspiration','num-of-doors','body-style','drive-wheels','engine-location','wheel-base','length','width','height','curb-weight','engine-type','num-of-cylinders','engine-size','fuel-system','bore','stroke','compression-ratio','horsepower','peak-rpm','city-mpg','highway-mpg','price']

data.shape

type(data)

data.head()

data.isnull().sum()

data['symboling'].unique()

data['normalized-losses'].notnull()

data['normalized-losses'].unique()

for each in column:
  print(each, data[each].unique())

data['normalized-losses'].unique()

data.loc[data['normalized-losses'] == '?', 'normalized-losses'].count()

data.loc[data['num-of-doors'] == '?', 'num-of-doors'].count()

data.loc[data['horsepower'] == '?', 'horsepower'].count()

data.loc[data['peak-rpm'] == '?', 'peak-rpm'].count()

data.loc[data['price'] == '?', 'price'].count()

import numpy as np

data.replace('?', np.nan, inplace = True)

data.isnull().sum()

data.dropna()

data.shape

data.drop('normalized-losses',axis=1,inplace=True)
data

data.isnull().sum()

data

data.head()

from sklearn.impute import SimpleImputer

SI = SimpleImputer(strategy='mean')

data[['bore','stroke','horsepower','peak-rpm','price']] = SI.fit_transform(data[['bore','stroke','horsepower','peak-rpm','price']])

data.isnull().sum()

data

SI = SimpleImputer(strategy='most_frequent')

data[['num-of-doors']] = SI.fit_transform(data[['num-of-doors']])

data

data.isnull().sum()

data_numeric = data.select_dtypes(include=['float64', 'int'])
data_numeric.head()

plt.figure(figsize=(20, 10))
sns.pairplot(data_numeric)
plt.show()

cor = data_numeric.corr()
cor

plt.figure(figsize=(16,8))

sns.heatmap(cor, cmap="YlGnBu", annot=True)
plt.show()

data.info()

data

data['symboling'] = data['symboling'].astype('object')
data.info()

data['make'][:30]

make = data['make'].apply(lambda x: x.split(" ")[0])
make[:30]

import re
p = re.compile(r'\w+-?\w+')
make = data['make'].apply(lambda x: re.findall(p, x)[0])
print(make)

data['car_company'] = data['make'].apply(lambda x: re.findall(p, x)[0])

data['car_company'].astype('category').value_counts()

data.loc[(data['car_company'] == "vw") | 
         (data['car_company'] == "vokswagen")
         , 'car_company'] = 'volkswagen'

# porsche
data.loc[data['car_company'] == "porcshce", 'car_company'] = 'porsche'

# toyota
data.loc[data['car_company'] == "toyouta", 'car_company'] = 'toyota'

# nissan
data.loc[data['car_company'] == "Nissan", 'car_company'] = 'nissan'

# mazda
data.loc[data['car_company'] == "maxda", 'car_company'] = 'mazda'

data['car_company'].astype('category').value_counts()

data = data.drop('make', axis=1)

data.info()

data.describe()

data

X = data.loc[:,['symboling','fuel-type','aspiration','num-of-doors','body-style','drive-wheels','engine-location','wheel-base','length','width','height','curb-weight','engine-type','num-of-cylinders','engine-size','fuel-system','bore','stroke','compression-ratio','horsepower','peak-rpm','city-mpg','highway-mpg','car_company']]

X

X.shape

y = data.iloc[:,-2]

y

data_categorical = X.select_dtypes(include=['object'])
data_categorical.head()

data_dummies = pd.get_dummies(data_categorical, drop_first=True)
data_dummies.head()

X = X.drop(list(data_categorical.columns), axis=1)

X = pd.concat([X, data_dummies], axis=1)

from sklearn.preprocessing import scale

cols = X.columns
X = pd.DataFrame(scale(X))
X.columns = cols
X.columns

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.2,random_state = 100)

X_train

from sklearn.linear_model import LinearRegression

lm = LinearRegression()

lm.fit(X_train, y_train)

print(lm.coef_)
print(lm.intercept_)

y_pred = lm.predict(X_test)

from sklearn.metrics import r2_score

print(r2_score(y_true=y_test, y_pred=y_pred))

from sklearn.feature_selection import RFE

lm = LinearRegression()
rfe_15 = RFE(lm,n_features_to_select=15)

rfe_15.fit(X_train, y_train)

print(rfe_15.support_)           
print(rfe_15.ranking_)

y_pred = rfe_15.predict(X_test)

print(r2_score(y_test, y_pred))

from sklearn.feature_selection import RFE

lm = LinearRegression()
rfe_6 = RFE(lm, n_features_to_select=6)

rfe_6.fit(X_train, y_train)
y_pred = rfe_6.predict(X_test)
print(r2_score(y_test, y_pred))

import statsmodels.api as sm  
col_15 = X_train.columns[rfe_15.support_]
X_train_rfe_15 = X_train[col_15]

X_train_rfe_15 = sm.add_constant(X_train_rfe_15)
X_train_rfe_15.head()

lm_15 = sm.OLS(y_train, X_train_rfe_15).fit()   
print(lm_15.summary())

X_test_rfe_15 = X_test[col_15]

X_test_rfe_15 = sm.add_constant(X_test_rfe_15, has_constant='add')
X_test_rfe_15.info()

y_pred = lm_15.predict(X_test_rfe_15)

r2_score(y_test, y_pred)

col_6 = X_train.columns[rfe_6.support_]

X_train_rfe_6 = X_train[col_6]

X_train_rfe_6 = sm.add_constant(X_train_rfe_6)

lm_6 = sm.OLS(y_train, X_train_rfe_6).fit()   
print(lm_6.summary())

X_test_rfe_6 = X_test[col_6]

X_test_rfe_6 = sm.add_constant(X_test_rfe_6, has_constant='add')
X_test_rfe_6.info()

y_pred = lm_6.predict(X_test_rfe_6)

r2_score(y_test, y_pred)

n_features_list = list(range(4, 20))
adjusted_r2 = []
r2 = []
test_r2 = []

for n_features in range(4, 20):

    lm = LinearRegression()

    
    rfe_n = RFE(lm, n_features_to_select=n_features)

    
    rfe_n.fit(X_train, y_train)

   
    col_n = X_train.columns[rfe_n.support_]

    
    X_train_rfe_n = X_train[col_n]

    
    X_train_rfe_n = sm.add_constant(X_train_rfe_n)


   
    lm_n = sm.OLS(y_train, X_train_rfe_n).fit()
    adjusted_r2.append(lm_n.rsquared_adj)
    r2.append(lm_n.rsquared)
    
    
    
    X_test_rfe_n = X_test[col_n]


    
    X_test_rfe_n = sm.add_constant(X_test_rfe_n, has_constant='add')



    
    y_pred = lm_n.predict(X_test_rfe_n)
    
    test_r2.append(r2_score(y_test, y_pred))

# final model
lm = LinearRegression()

rfe_n = RFE(lm, n_features_to_select=6)


rfe_n.fit(X_train, y_train)


col_n = X_train.columns[rfe_n.support_]

X_train_rfe_n = X_train[col_n]

X_train_rfe_n = sm.add_constant(X_train_rfe_n)


lm_n = sm.OLS(y_train, X_train_rfe_n).fit()
adjusted_r2.append(lm_n.rsquared_adj)
r2.append(lm_n.rsquared)


X_test_rfe_n = X_test[col_n]


X_test_rfe_n = sm.add_constant(X_test_rfe_n, has_constant='add')


y_pred = lm_n.predict(X_test_rfe_n)

test_r2.append(r2_score(y_test, y_pred))

# summary
lm_n.summary()

# results 
r2_score(y_test, y_pred)

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

mae = mean_absolute_error(y_test, y_pred)
print("MAE without outliers:", mae)
mse = mean_squared_error(y_test, y_pred)
print("MSE without outliers:", mse)
#output
MAE without outliers: 2331.4841809803816
MSE without outliers: 11519060.225614587
r2 score:0.8504771200231704
